{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6201722",
   "metadata": {},
   "source": [
    "                                Mini Project 12_ANN (Heart Failure Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff739cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import yellowbrick\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.tree import plot_tree\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib.collections import PathCollection\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# --- Libraries Settings ---\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi']=100\n",
    "     \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({\"font.size\": 13, \"figure.figsize\": [8,4]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef5bae",
   "metadata": {},
   "source": [
    "# 1. Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97db6e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0  75.0        0                       582         0                 20   \n",
       "1  55.0        0                      7861         0                 38   \n",
       "2  65.0        0                       146         0                 20   \n",
       "3  50.0        1                       111         0                 20   \n",
       "4  65.0        1                       160         1                 20   \n",
       "\n",
       "   high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                    1  265000.00               1.9           130    1   \n",
       "1                    0  263358.03               1.1           136    1   \n",
       "2                    0  162000.00               1.3           129    1   \n",
       "3                    0  210000.00               1.9           137    1   \n",
       "4                    0  327000.00               2.7           116    0   \n",
       "\n",
       "   smoking  time  DEATH_EVENT  \n",
       "0        0     4            1  \n",
       "1        0     6            1  \n",
       "2        1     7            1  \n",
       "3        0     7            1  \n",
       "4        0     8            1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(\"heart_failure_clinical_records_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22986e",
   "metadata": {},
   "source": [
    "Here, the target variable is `DEATH_EVENT`. That means, it is a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34a593",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d129f4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 299 entries, 0 to 298\n",
      "Data columns (total 13 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   age                       299 non-null    float64\n",
      " 1   anaemia                   299 non-null    int64  \n",
      " 2   creatinine_phosphokinase  299 non-null    int64  \n",
      " 3   diabetes                  299 non-null    int64  \n",
      " 4   ejection_fraction         299 non-null    int64  \n",
      " 5   high_blood_pressure       299 non-null    int64  \n",
      " 6   platelets                 299 non-null    float64\n",
      " 7   serum_creatinine          299 non-null    float64\n",
      " 8   serum_sodium              299 non-null    int64  \n",
      " 9   sex                       299 non-null    int64  \n",
      " 10  smoking                   299 non-null    int64  \n",
      " 11  time                      299 non-null    int64  \n",
      " 12  DEATH_EVENT               299 non-null    int64  \n",
      "dtypes: float64(3), int64(10)\n",
      "memory usage: 30.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a3715",
   "metadata": {},
   "source": [
    "From dataset description, we can see that:\n",
    "- The shape of the dataset is (299, 13). \n",
    "- There are no missing values.\n",
    "- The data type of `age` is float. We'll convert it to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448029d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age\"] = df[\"age\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fc772f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>time</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "      <td>299.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.8294</td>\n",
       "      <td>0.4314</td>\n",
       "      <td>581.8395</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>38.0836</td>\n",
       "      <td>0.3512</td>\n",
       "      <td>263358.0293</td>\n",
       "      <td>1.3939</td>\n",
       "      <td>136.6254</td>\n",
       "      <td>0.6488</td>\n",
       "      <td>0.3211</td>\n",
       "      <td>130.2609</td>\n",
       "      <td>0.3211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.8950</td>\n",
       "      <td>0.4961</td>\n",
       "      <td>970.2879</td>\n",
       "      <td>0.4941</td>\n",
       "      <td>11.8348</td>\n",
       "      <td>0.4781</td>\n",
       "      <td>97804.2369</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>4.4125</td>\n",
       "      <td>0.4781</td>\n",
       "      <td>0.4677</td>\n",
       "      <td>77.6142</td>\n",
       "      <td>0.4677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>23.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25100.0000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>113.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>51.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>116.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>212500.0000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>134.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>73.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>250.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>38.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>262000.0000</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>137.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>115.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>582.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>45.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>303500.0000</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>140.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>203.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>95.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>7861.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>850000.0000</td>\n",
       "      <td>9.4000</td>\n",
       "      <td>148.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>285.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age   anaemia  creatinine_phosphokinase  diabetes  \\\n",
       "count  299.0000  299.0000                  299.0000  299.0000   \n",
       "mean    60.8294    0.4314                  581.8395    0.4181   \n",
       "std     11.8950    0.4961                  970.2879    0.4941   \n",
       "min     40.0000    0.0000                   23.0000    0.0000   \n",
       "25%     51.0000    0.0000                  116.5000    0.0000   \n",
       "50%     60.0000    0.0000                  250.0000    0.0000   \n",
       "75%     70.0000    1.0000                  582.0000    1.0000   \n",
       "max     95.0000    1.0000                 7861.0000    1.0000   \n",
       "\n",
       "       ejection_fraction  high_blood_pressure    platelets  serum_creatinine  \\\n",
       "count           299.0000             299.0000     299.0000          299.0000   \n",
       "mean             38.0836               0.3512  263358.0293            1.3939   \n",
       "std              11.8348               0.4781   97804.2369            1.0345   \n",
       "min              14.0000               0.0000   25100.0000            0.5000   \n",
       "25%              30.0000               0.0000  212500.0000            0.9000   \n",
       "50%              38.0000               0.0000  262000.0000            1.1000   \n",
       "75%              45.0000               1.0000  303500.0000            1.4000   \n",
       "max              80.0000               1.0000  850000.0000            9.4000   \n",
       "\n",
       "       serum_sodium       sex   smoking      time  DEATH_EVENT  \n",
       "count      299.0000  299.0000  299.0000  299.0000     299.0000  \n",
       "mean       136.6254    0.6488    0.3211  130.2609       0.3211  \n",
       "std          4.4125    0.4781    0.4677   77.6142       0.4677  \n",
       "min        113.0000    0.0000    0.0000    4.0000       0.0000  \n",
       "25%        134.0000    0.0000    0.0000   73.0000       0.0000  \n",
       "50%        137.0000    1.0000    0.0000  115.0000       0.0000  \n",
       "75%        140.0000    1.0000    1.0000  203.0000       1.0000  \n",
       "max        148.0000    1.0000    1.0000  285.0000       1.0000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873baec1",
   "metadata": {},
   "source": [
    "From dataset description, we can see that:\n",
    "- `anaemia`, `diabetes`, `high_blood_pressure`, `sex`, `smoking` and `DEATH_EVENT` are qualitative variables. One easy way to find out is to check the min and max values of those variables. We need to decode these variables.\n",
    "- The rest are quantitative variables.\n",
    "- Age is distributed between 40 to 95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cacdd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.6789\n",
       "1    0.3211\n",
       "Name: DEATH_EVENT, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"DEATH_EVENT\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72994a95",
   "metadata": {},
   "source": [
    "We have approximately 68% False and 32% True values for DEATH_EVENT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33827f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.6488\n",
       "0    0.3512\n",
       "Name: sex, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sex\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f6942",
   "metadata": {},
   "source": [
    "We have approximately 65% Male and 35% Female in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed6444a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(df[\"age\"], bins=30)\n",
    "plt.title(\"Age Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e95e67",
   "metadata": {},
   "source": [
    "For age, the median is around 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd6ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,0:-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bedeb3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Encode the 'sex' column\n",
    "sex_encoded = encoder.fit_transform(df[['sex']])\n",
    "\n",
    "# Convert the encoded column to a dense array\n",
    "sex_array = sex_encoded.toarray()\n",
    "\n",
    "# Replace the 'sex' column with the encoded array\n",
    "df = df.drop(columns=['sex'])\n",
    "df['male'] = sex_array[:, 1]  # Assuming '1' represents 'male'\n",
    "\n",
    "# Now, you can proceed with StandardScaler\n",
    "x = df.iloc[:, :-1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904e8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68486b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "      #  keras.layers.Flatten()\n",
    "      keras.layers.Dense(units=16, activation=tf.keras.activations.relu, input_dim=12, name='Input_Layer'),\n",
    "      keras.layers.Dense(units=8, activation=tf.keras.activations.relu, name='Hidden_Layer_1'),\n",
    "      keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid, name='Output_Layer')\n",
    "    ]\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48799a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_Layer (Dense)         (None, 16)                208       \n",
      "                                                                 \n",
      " Hidden_Layer_1 (Dense)      (None, 8)                 136       \n",
      "                                                                 \n",
      " Output_Layer (Dense)        (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 353 (1.38 KB)\n",
      "Trainable params: 353 (1.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b519973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "230e0dd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "8/8 [==============================] - 1s 33ms/step - loss: 6279.6895 - accuracy: 0.3138 - val_loss: 1512.8635 - val_accuracy: 0.3500\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 882.6599 - accuracy: 0.5941 - val_loss: 1295.0532 - val_accuracy: 0.6500\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1376.7650 - accuracy: 0.6862 - val_loss: 872.9571 - val_accuracy: 0.6500\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 496.5286 - accuracy: 0.5732 - val_loss: 437.7155 - val_accuracy: 0.3500\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 215.5955 - accuracy: 0.5649 - val_loss: 156.7959 - val_accuracy: 0.6500\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 104.0648 - accuracy: 0.5272 - val_loss: 66.7311 - val_accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 78.2795 - accuracy: 0.5439 - val_loss: 82.6222 - val_accuracy: 0.6500\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 58.8359 - accuracy: 0.5983 - val_loss: 9.2324 - val_accuracy: 0.6333\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79.2294 - accuracy: 0.5439 - val_loss: 10.0607 - val_accuracy: 0.6000\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 30.4553 - accuracy: 0.5690 - val_loss: 20.1481 - val_accuracy: 0.3667\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 91.7038 - accuracy: 0.5439 - val_loss: 20.5317 - val_accuracy: 0.6833\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 31.4038 - accuracy: 0.5858 - val_loss: 18.3305 - val_accuracy: 0.6833\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 30.5427 - accuracy: 0.5774 - val_loss: 11.4793 - val_accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 72.5424 - accuracy: 0.5565 - val_loss: 8.8431 - val_accuracy: 0.6833\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 34.1969 - accuracy: 0.5900 - val_loss: 27.1717 - val_accuracy: 0.3500\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 39.9340 - accuracy: 0.5774 - val_loss: 10.7529 - val_accuracy: 0.4333\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 97.9508 - accuracy: 0.5314 - val_loss: 21.5937 - val_accuracy: 0.6667\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56.4861 - accuracy: 0.5481 - val_loss: 78.5338 - val_accuracy: 0.6500\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 78.6553 - accuracy: 0.5816 - val_loss: 111.3458 - val_accuracy: 0.6500\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 62.3550 - accuracy: 0.6234 - val_loss: 15.9542 - val_accuracy: 0.6667\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 72.6130 - accuracy: 0.5732 - val_loss: 75.0714 - val_accuracy: 0.3500\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 69.3258 - accuracy: 0.4979 - val_loss: 45.8710 - val_accuracy: 0.6667\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 74.2636 - accuracy: 0.5900 - val_loss: 30.1179 - val_accuracy: 0.6500\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 30.9849 - accuracy: 0.6109 - val_loss: 10.0356 - val_accuracy: 0.6833\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 34.7813 - accuracy: 0.5816 - val_loss: 10.6715 - val_accuracy: 0.6833\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25.7943 - accuracy: 0.5816 - val_loss: 60.1905 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 52.7243 - accuracy: 0.6360 - val_loss: 66.0273 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 93.2674 - accuracy: 0.5021 - val_loss: 203.6333 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 164.3191 - accuracy: 0.6192 - val_loss: 201.0504 - val_accuracy: 0.3500\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 95.1150 - accuracy: 0.5314 - val_loss: 36.5796 - val_accuracy: 0.6667\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 66.4806 - accuracy: 0.5816 - val_loss: 108.7979 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 82.5285 - accuracy: 0.5900 - val_loss: 45.5554 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 69.2521 - accuracy: 0.5649 - val_loss: 44.1699 - val_accuracy: 0.3500\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 57.2033 - accuracy: 0.5021 - val_loss: 44.5680 - val_accuracy: 0.6500\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 60.9442 - accuracy: 0.5690 - val_loss: 20.2427 - val_accuracy: 0.6667\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 34.6994 - accuracy: 0.6444 - val_loss: 1.5296 - val_accuracy: 0.8167\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 19.8518 - accuracy: 0.6402 - val_loss: 63.5731 - val_accuracy: 0.3500\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 31.0894 - accuracy: 0.5021 - val_loss: 10.4487 - val_accuracy: 0.6667\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 21.0894 - accuracy: 0.5900 - val_loss: 81.2499 - val_accuracy: 0.6500\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 135.0774 - accuracy: 0.6736 - val_loss: 248.2223 - val_accuracy: 0.3500\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 127.1065 - accuracy: 0.6611 - val_loss: 88.1228 - val_accuracy: 0.6500\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 124.5824 - accuracy: 0.5272 - val_loss: 213.8208 - val_accuracy: 0.6500\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 182.5156 - accuracy: 0.4519 - val_loss: 97.5879 - val_accuracy: 0.6500\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 77.8359 - accuracy: 0.5649 - val_loss: 52.8373 - val_accuracy: 0.6500\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 61.7645 - accuracy: 0.6067 - val_loss: 1.5114 - val_accuracy: 0.8167\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 29.7169 - accuracy: 0.5690 - val_loss: 33.4749 - val_accuracy: 0.6500\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 19.5929 - accuracy: 0.6234 - val_loss: 26.1993 - val_accuracy: 0.6500\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 23.9769 - accuracy: 0.6025 - val_loss: 9.0425 - val_accuracy: 0.4167\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 49.1243 - accuracy: 0.5481 - val_loss: 76.1964 - val_accuracy: 0.6500\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 31.8059 - accuracy: 0.6318 - val_loss: 21.4928 - val_accuracy: 0.6500\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 52.4452 - accuracy: 0.6318 - val_loss: 128.2019 - val_accuracy: 0.6500\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 55.4682 - accuracy: 0.5816 - val_loss: 28.5088 - val_accuracy: 0.6500\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 27.2607 - accuracy: 0.5941 - val_loss: 43.2526 - val_accuracy: 0.6500\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 50.2194 - accuracy: 0.6109 - val_loss: 18.9661 - val_accuracy: 0.6500\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 13ms/step - loss: 74.6570 - accuracy: 0.5439 - val_loss: 160.4476 - val_accuracy: 0.3500\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 145.4201 - accuracy: 0.5272 - val_loss: 27.8828 - val_accuracy: 0.6500\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 93.5494 - accuracy: 0.5607 - val_loss: 87.4601 - val_accuracy: 0.6500\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 77.7679 - accuracy: 0.5649 - val_loss: 166.8059 - val_accuracy: 0.6500\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 122.4358 - accuracy: 0.5397 - val_loss: 139.6120 - val_accuracy: 0.6500\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 147.7411 - accuracy: 0.6151 - val_loss: 134.2526 - val_accuracy: 0.3500\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 69.4443 - accuracy: 0.6109 - val_loss: 17.7797 - val_accuracy: 0.6500\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 55.9372 - accuracy: 0.6067 - val_loss: 73.6494 - val_accuracy: 0.6500\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 99.2981 - accuracy: 0.5523 - val_loss: 178.4052 - val_accuracy: 0.6500\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 107.2963 - accuracy: 0.5774 - val_loss: 120.0736 - val_accuracy: 0.6500\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 116.4795 - accuracy: 0.5816 - val_loss: 69.8810 - val_accuracy: 0.6500\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 142.5700 - accuracy: 0.6653 - val_loss: 175.3217 - val_accuracy: 0.3500\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 137.2940 - accuracy: 0.5732 - val_loss: 2.1932 - val_accuracy: 0.8333\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 138.7117 - accuracy: 0.5105 - val_loss: 129.2001 - val_accuracy: 0.6500\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 130.2478 - accuracy: 0.5272 - val_loss: 104.3614 - val_accuracy: 0.6500\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 91.4117 - accuracy: 0.6234 - val_loss: 42.3011 - val_accuracy: 0.6500\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 73.2998 - accuracy: 0.6067 - val_loss: 47.8989 - val_accuracy: 0.6500\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 49.2221 - accuracy: 0.6192 - val_loss: 42.9116 - val_accuracy: 0.6500\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 42.2961 - accuracy: 0.5607 - val_loss: 15.5007 - val_accuracy: 0.4167\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 141.0528 - accuracy: 0.5858 - val_loss: 128.1530 - val_accuracy: 0.3500\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 110.1212 - accuracy: 0.6276 - val_loss: 160.3566 - val_accuracy: 0.3500\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 87.8191 - accuracy: 0.4728 - val_loss: 6.7358 - val_accuracy: 0.6667\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 15.6511 - accuracy: 0.6151 - val_loss: 2.4356 - val_accuracy: 0.8333\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 37.1570 - accuracy: 0.6192 - val_loss: 122.2984 - val_accuracy: 0.3500\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 144.6670 - accuracy: 0.6192 - val_loss: 53.2120 - val_accuracy: 0.6500\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 121.2609 - accuracy: 0.5272 - val_loss: 90.7016 - val_accuracy: 0.6500\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 88.8077 - accuracy: 0.5649 - val_loss: 123.5176 - val_accuracy: 0.6500\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 98.4842 - accuracy: 0.5732 - val_loss: 130.1968 - val_accuracy: 0.6500\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 84.6676 - accuracy: 0.6109 - val_loss: 137.1544 - val_accuracy: 0.6500\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 131.4634 - accuracy: 0.6695 - val_loss: 49.2564 - val_accuracy: 0.3500\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 109.4006 - accuracy: 0.6067 - val_loss: 114.7236 - val_accuracy: 0.3500\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 97.1530 - accuracy: 0.5565 - val_loss: 42.6128 - val_accuracy: 0.3500\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 82.7239 - accuracy: 0.5439 - val_loss: 44.6172 - val_accuracy: 0.3500\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 140.1980 - accuracy: 0.6151 - val_loss: 79.0554 - val_accuracy: 0.3500\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 131.9654 - accuracy: 0.4644 - val_loss: 46.7424 - val_accuracy: 0.3500\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 47.8862 - accuracy: 0.5063 - val_loss: 40.6868 - val_accuracy: 0.6500\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 78.3209 - accuracy: 0.5481 - val_loss: 83.6614 - val_accuracy: 0.6500\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 115.3603 - accuracy: 0.6067 - val_loss: 84.2576 - val_accuracy: 0.3500\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 91.7344 - accuracy: 0.6527 - val_loss: 225.4249 - val_accuracy: 0.3500\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 185.5387 - accuracy: 0.5230 - val_loss: 215.3333 - val_accuracy: 0.6500\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 112.9216 - accuracy: 0.5732 - val_loss: 101.3208 - val_accuracy: 0.6500\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 86.2357 - accuracy: 0.5774 - val_loss: 88.2598 - val_accuracy: 0.6500\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 58.6067 - accuracy: 0.6067 - val_loss: 49.7822 - val_accuracy: 0.6500\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 23.3252 - accuracy: 0.6360 - val_loss: 93.4718 - val_accuracy: 0.3500\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 74.2078 - accuracy: 0.5732 - val_loss: 29.7874 - val_accuracy: 0.3667\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 50.7535 - accuracy: 0.6151 - val_loss: 31.2404 - val_accuracy: 0.6500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert input features and target labels to appropriate data types\n",
    "x_train = np.array(x_train, dtype=np.float32)\n",
    "x_test = np.array(x_test, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.int32)\n",
    "y_test = np.array(y_test, dtype=np.int32)\n",
    "\n",
    "# Then, train the model\n",
    "hist = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5589c96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 36.3580 - accuracy: 0.6862\n",
      "Training Accuracy: 68.62%\n",
      "\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 31.2404 - accuracy: 0.6500\n",
      "Testing Accuracy: 65.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and Test accuracy\n",
    "scores = model.evaluate(x_train, y_train)\n",
    "print(\"Training Accuracy: %.2f%%\\n\" % (scores[1] * 100))\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"Testing Accuracy: %.2f%%\\n\" % (scores[1] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c50fe13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                416       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 961 (3.75 KB)\n",
      "Trainable params: 961 (3.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(units=32, activation='relu', input_dim=12),\n",
    "    keras.layers.Dropout(0.2),  # Adding dropout for regularization\n",
    "    keras.layers.Dense(units=16, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),  # Adding dropout for regularization\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfd7eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: 16829.5898 - accuracy: 0.6736 - val_loss: 10907.7051 - val_accuracy: 0.6500\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 14474.0586 - accuracy: 0.6360 - val_loss: 6810.9253 - val_accuracy: 0.6500\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 11642.2334 - accuracy: 0.5649 - val_loss: 3503.2156 - val_accuracy: 0.6500\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7902.0352 - accuracy: 0.6025 - val_loss: 1798.3654 - val_accuracy: 0.6500\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 8170.7715 - accuracy: 0.5690 - val_loss: 1506.4128 - val_accuracy: 0.6500\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 8085.4077 - accuracy: 0.5565 - val_loss: 1687.4719 - val_accuracy: 0.6500\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7099.2793 - accuracy: 0.5858 - val_loss: 1645.7643 - val_accuracy: 0.6500\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 6773.3213 - accuracy: 0.5690 - val_loss: 1383.0712 - val_accuracy: 0.6500\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 6132.3950 - accuracy: 0.6151 - val_loss: 667.9714 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5047.7705 - accuracy: 0.5397 - val_loss: 239.8941 - val_accuracy: 0.6500\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 6512.7817 - accuracy: 0.5105 - val_loss: 377.8523 - val_accuracy: 0.6500\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 4708.0156 - accuracy: 0.5900 - val_loss: 520.5198 - val_accuracy: 0.6500\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5575.1367 - accuracy: 0.5230 - val_loss: 355.9030 - val_accuracy: 0.6500\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 4818.7114 - accuracy: 0.5565 - val_loss: 117.3036 - val_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3932.3821 - accuracy: 0.5607 - val_loss: 302.8478 - val_accuracy: 0.6500\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3066.2773 - accuracy: 0.5481 - val_loss: 400.3950 - val_accuracy: 0.6500\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3342.0923 - accuracy: 0.5105 - val_loss: 432.9756 - val_accuracy: 0.6500\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3805.1670 - accuracy: 0.5607 - val_loss: 541.0008 - val_accuracy: 0.6500\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3196.5515 - accuracy: 0.5816 - val_loss: 423.7543 - val_accuracy: 0.6500\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2684.9949 - accuracy: 0.5732 - val_loss: 244.6386 - val_accuracy: 0.6500\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 2112.6172 - accuracy: 0.6360 - val_loss: 201.1712 - val_accuracy: 0.6500\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2259.9958 - accuracy: 0.5941 - val_loss: 113.2184 - val_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2106.4143 - accuracy: 0.5188 - val_loss: 306.2046 - val_accuracy: 0.6500\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2464.7253 - accuracy: 0.5816 - val_loss: 328.3370 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1810.3219 - accuracy: 0.5858 - val_loss: 367.9834 - val_accuracy: 0.6500\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1846.6653 - accuracy: 0.6151 - val_loss: 342.1125 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1691.7772 - accuracy: 0.4979 - val_loss: 183.1093 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1145.3954 - accuracy: 0.6485 - val_loss: 227.3870 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1798.5264 - accuracy: 0.5021 - val_loss: 196.0601 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1654.4427 - accuracy: 0.5021 - val_loss: 217.7263 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1488.0869 - accuracy: 0.5439 - val_loss: 154.4207 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1461.2434 - accuracy: 0.5732 - val_loss: 48.0379 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1113.5262 - accuracy: 0.5356 - val_loss: 71.2431 - val_accuracy: 0.6500\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1221.4996 - accuracy: 0.5481 - val_loss: 41.9171 - val_accuracy: 0.6500\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1453.0012 - accuracy: 0.5439 - val_loss: 51.9058 - val_accuracy: 0.6500\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1105.4845 - accuracy: 0.5732 - val_loss: 77.3509 - val_accuracy: 0.6500\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1008.1772 - accuracy: 0.5774 - val_loss: 57.3885 - val_accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 866.2609 - accuracy: 0.6067 - val_loss: 37.6896 - val_accuracy: 0.6500\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 869.1591 - accuracy: 0.5481 - val_loss: 23.1236 - val_accuracy: 0.6500\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1145.8712 - accuracy: 0.5481 - val_loss: 13.2799 - val_accuracy: 0.6500\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 780.7891 - accuracy: 0.5690 - val_loss: 6.5684 - val_accuracy: 0.6500\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 708.1766 - accuracy: 0.5774 - val_loss: 5.1315 - val_accuracy: 0.3500\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1227.1515 - accuracy: 0.5105 - val_loss: 12.2366 - val_accuracy: 0.3500\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 818.1909 - accuracy: 0.4728 - val_loss: 0.9267 - val_accuracy: 0.6500\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 662.0836 - accuracy: 0.6527 - val_loss: 2.9623 - val_accuracy: 0.6500\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 661.1556 - accuracy: 0.5690 - val_loss: 1.1460 - val_accuracy: 0.3500\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 707.2200 - accuracy: 0.5021 - val_loss: 5.6922 - val_accuracy: 0.3500\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 575.8893 - accuracy: 0.5481 - val_loss: 2.3627 - val_accuracy: 0.6500\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 449.3660 - accuracy: 0.6276 - val_loss: 0.6341 - val_accuracy: 0.6500\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 590.1836 - accuracy: 0.5439 - val_loss: 2.3285 - val_accuracy: 0.3500\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 371.9917 - accuracy: 0.5397 - val_loss: 0.6287 - val_accuracy: 0.6500\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 452.3896 - accuracy: 0.6234 - val_loss: 0.8874 - val_accuracy: 0.3500\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 560.5977 - accuracy: 0.6151 - val_loss: 1.2245 - val_accuracy: 0.6500\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 371.0231 - accuracy: 0.6527 - val_loss: 0.8679 - val_accuracy: 0.3500\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 462.7289 - accuracy: 0.4644 - val_loss: 0.7725 - val_accuracy: 0.6500\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 574.1622 - accuracy: 0.6109 - val_loss: 1.0377 - val_accuracy: 0.6500\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 375.7293 - accuracy: 0.5900 - val_loss: 1.2901 - val_accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 313.4239 - accuracy: 0.5607 - val_loss: 0.6601 - val_accuracy: 0.6500\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 323.7690 - accuracy: 0.6820 - val_loss: 0.7389 - val_accuracy: 0.3500\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 303.9321 - accuracy: 0.5481 - val_loss: 0.6513 - val_accuracy: 0.6500\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 374.1459 - accuracy: 0.6485 - val_loss: 1.0591 - val_accuracy: 0.3500\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 169.5363 - accuracy: 0.5523 - val_loss: 0.6983 - val_accuracy: 0.6500\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 254.2679 - accuracy: 0.6862 - val_loss: 0.6432 - val_accuracy: 0.6500\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 254.2020 - accuracy: 0.5941 - val_loss: 0.6375 - val_accuracy: 0.6500\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 370.5765 - accuracy: 0.5565 - val_loss: 0.6371 - val_accuracy: 0.6500\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 153.6783 - accuracy: 0.6611 - val_loss: 0.9305 - val_accuracy: 0.6500\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 227.1331 - accuracy: 0.6611 - val_loss: 0.6283 - val_accuracy: 0.6500\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 171.4105 - accuracy: 0.6360 - val_loss: 0.7009 - val_accuracy: 0.3000\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 118.2740 - accuracy: 0.6318 - val_loss: 0.6384 - val_accuracy: 0.6500\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 165.9548 - accuracy: 0.6611 - val_loss: 0.6530 - val_accuracy: 0.6500\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 108.6510 - accuracy: 0.6653 - val_loss: 0.6451 - val_accuracy: 0.6500\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 61.0351 - accuracy: 0.6904 - val_loss: 0.6854 - val_accuracy: 0.6500\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 211.9752 - accuracy: 0.6318 - val_loss: 0.6586 - val_accuracy: 0.6500\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 142.2338 - accuracy: 0.6862 - val_loss: 0.6690 - val_accuracy: 0.6500\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 138.8141 - accuracy: 0.6485 - val_loss: 0.6911 - val_accuracy: 0.5333\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 114.6073 - accuracy: 0.6611 - val_loss: 0.6731 - val_accuracy: 0.6500\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 117.4525 - accuracy: 0.6653 - val_loss: 0.6757 - val_accuracy: 0.6500\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 104.8526 - accuracy: 0.6402 - val_loss: 0.6756 - val_accuracy: 0.6500\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 137.9766 - accuracy: 0.5774 - val_loss: 0.6745 - val_accuracy: 0.6500\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 106.1178 - accuracy: 0.6695 - val_loss: 0.6741 - val_accuracy: 0.6500\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 77.8569 - accuracy: 0.6444 - val_loss: 0.6732 - val_accuracy: 0.6500\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 96.8350 - accuracy: 0.6527 - val_loss: 0.6723 - val_accuracy: 0.6500\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 81.7170 - accuracy: 0.6736 - val_loss: 0.6716 - val_accuracy: 0.6500\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 108.0117 - accuracy: 0.6067 - val_loss: 0.6709 - val_accuracy: 0.6500\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 97.6570 - accuracy: 0.6569 - val_loss: 0.6704 - val_accuracy: 0.6500\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 100.4135 - accuracy: 0.6611 - val_loss: 0.6701 - val_accuracy: 0.6500\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 205.7318 - accuracy: 0.6527 - val_loss: 0.6698 - val_accuracy: 0.6500\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 44.2663 - accuracy: 0.6360 - val_loss: 0.6693 - val_accuracy: 0.6500\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57.0783 - accuracy: 0.6444 - val_loss: 0.6689 - val_accuracy: 0.6500\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 91.0582 - accuracy: 0.6653 - val_loss: 0.6684 - val_accuracy: 0.6500\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 98.1993 - accuracy: 0.6444 - val_loss: 0.6680 - val_accuracy: 0.6500\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 96.8607 - accuracy: 0.6736 - val_loss: 0.6678 - val_accuracy: 0.6500\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 55.0566 - accuracy: 0.6695 - val_loss: 0.6676 - val_accuracy: 0.6500\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 25.8981 - accuracy: 0.6276 - val_loss: 0.6670 - val_accuracy: 0.6500\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 150.9094 - accuracy: 0.6569 - val_loss: 0.6666 - val_accuracy: 0.6500\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 88.7277 - accuracy: 0.6234 - val_loss: 0.6662 - val_accuracy: 0.6500\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 142.4876 - accuracy: 0.6527 - val_loss: 0.6658 - val_accuracy: 0.6500\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 49.7190 - accuracy: 0.6444 - val_loss: 0.6654 - val_accuracy: 0.6500\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 68.3327 - accuracy: 0.6904 - val_loss: 0.6650 - val_accuracy: 0.6500\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 36.5322 - accuracy: 0.6862 - val_loss: 0.6645 - val_accuracy: 0.6500\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc3bd2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6559 - accuracy: 0.6862\n",
      "Training Accuracy: 0.69\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6645 - accuracy: 0.6500\n",
      "Testing Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "train_loss, train_accuracy = model.evaluate(x_train, y_train)\n",
    "print(f'Training Accuracy: {train_accuracy:.2f}')\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Testing Accuracy: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d5547",
   "metadata": {},
   "source": [
    "Having a higher training accuracy compared to testing accuracy suggests that the model might be overfitting to the training data, meaning it's performing well on the training data but not generalizing well to unseen data.\n",
    "\n",
    "To improve the model's performance and address potential overfitting, we can further tune hyperparameters, adjust the model architecture, or apply regularization techniques like dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df4bbfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 32)                416       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 961 (3.75 KB)\n",
      "Trainable params: 961 (3.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model with dropout regularization\n",
    "model_with_dropout = keras.Sequential([\n",
    "    keras.layers.Dense(units=32, activation='relu', input_dim=12),\n",
    "    keras.layers.Dropout(0.2),  # Dropout rate of 0.2 after the first hidden layer\n",
    "    keras.layers.Dense(units=16, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),  # Dropout rate of 0.2 after the second hidden layer\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_with_dropout.compile(optimizer='adam',\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_with_dropout.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98c83920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 29ms/step - loss: 21755.4727 - accuracy: 0.3305 - val_loss: 16760.1836 - val_accuracy: 0.3500\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 12985.9307 - accuracy: 0.3808 - val_loss: 11202.4463 - val_accuracy: 0.3500\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 9067.4473 - accuracy: 0.4519 - val_loss: 4232.4775 - val_accuracy: 0.3500\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7398.8115 - accuracy: 0.4770 - val_loss: 734.3812 - val_accuracy: 0.6500\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 5893.9819 - accuracy: 0.5774 - val_loss: 1977.0775 - val_accuracy: 0.6500\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 4927.3286 - accuracy: 0.6402 - val_loss: 1620.3918 - val_accuracy: 0.6500\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4926.8389 - accuracy: 0.5397 - val_loss: 886.2618 - val_accuracy: 0.6500\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3987.5825 - accuracy: 0.4937 - val_loss: 578.9199 - val_accuracy: 0.6500\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3730.8784 - accuracy: 0.5649 - val_loss: 458.7279 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4254.3604 - accuracy: 0.5230 - val_loss: 218.6558 - val_accuracy: 0.6500\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3418.9438 - accuracy: 0.5188 - val_loss: 444.9697 - val_accuracy: 0.6500\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3149.4976 - accuracy: 0.5732 - val_loss: 585.4252 - val_accuracy: 0.6500\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2720.0708 - accuracy: 0.5523 - val_loss: 661.2963 - val_accuracy: 0.6500\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2710.2744 - accuracy: 0.5314 - val_loss: 571.1135 - val_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2139.7173 - accuracy: 0.5565 - val_loss: 428.5672 - val_accuracy: 0.6500\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2866.7266 - accuracy: 0.5774 - val_loss: 301.3267 - val_accuracy: 0.6500\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1869.7312 - accuracy: 0.5858 - val_loss: 118.9654 - val_accuracy: 0.6500\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1692.4517 - accuracy: 0.5690 - val_loss: 0.6865 - val_accuracy: 0.6500\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1506.5859 - accuracy: 0.5732 - val_loss: 0.6862 - val_accuracy: 0.6500\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1663.8160 - accuracy: 0.6234 - val_loss: 0.6858 - val_accuracy: 0.6500\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 836.6958 - accuracy: 0.6444 - val_loss: 0.6855 - val_accuracy: 0.6500\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1070.8234 - accuracy: 0.6109 - val_loss: 0.6853 - val_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1185.5913 - accuracy: 0.6527 - val_loss: 0.6851 - val_accuracy: 0.6500\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 707.1660 - accuracy: 0.6485 - val_loss: 0.6847 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1031.9408 - accuracy: 0.6067 - val_loss: 0.6844 - val_accuracy: 0.6500\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 593.5948 - accuracy: 0.6862 - val_loss: 0.6840 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 523.7781 - accuracy: 0.6234 - val_loss: 0.6835 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 624.8412 - accuracy: 0.6569 - val_loss: 0.6829 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 518.8702 - accuracy: 0.6569 - val_loss: 0.6824 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 735.6799 - accuracy: 0.5983 - val_loss: 0.6819 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 683.5809 - accuracy: 0.6025 - val_loss: 0.6812 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 531.8922 - accuracy: 0.6402 - val_loss: 0.6806 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 736.2175 - accuracy: 0.6736 - val_loss: 0.6800 - val_accuracy: 0.6500\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 324.7978 - accuracy: 0.6402 - val_loss: 0.6794 - val_accuracy: 0.6500\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 317.2561 - accuracy: 0.6778 - val_loss: 0.6788 - val_accuracy: 0.6500\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 265.5748 - accuracy: 0.6653 - val_loss: 0.6783 - val_accuracy: 0.6500\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 262.8141 - accuracy: 0.6611 - val_loss: 0.6778 - val_accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 183.4523 - accuracy: 0.6736 - val_loss: 0.6773 - val_accuracy: 0.6500\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 162.5825 - accuracy: 0.6527 - val_loss: 0.6767 - val_accuracy: 0.6500\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 635.1422 - accuracy: 0.6485 - val_loss: 0.6759 - val_accuracy: 0.6500\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 153.4228 - accuracy: 0.6820 - val_loss: 0.6752 - val_accuracy: 0.6500\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 347.6258 - accuracy: 0.6820 - val_loss: 0.6746 - val_accuracy: 0.6500\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 125.1434 - accuracy: 0.6695 - val_loss: 0.6740 - val_accuracy: 0.6500\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 146.0477 - accuracy: 0.6653 - val_loss: 0.6734 - val_accuracy: 0.6500\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 117.9127 - accuracy: 0.6736 - val_loss: 0.6728 - val_accuracy: 0.6500\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 139.4595 - accuracy: 0.6695 - val_loss: 0.6722 - val_accuracy: 0.6500\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 183.5964 - accuracy: 0.6778 - val_loss: 0.6717 - val_accuracy: 0.6500\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 170.6771 - accuracy: 0.6820 - val_loss: 0.6711 - val_accuracy: 0.6500\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 265.5287 - accuracy: 0.6736 - val_loss: 0.6706 - val_accuracy: 0.6500\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 199.9406 - accuracy: 0.6611 - val_loss: 0.6701 - val_accuracy: 0.6500\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 154.2592 - accuracy: 0.6695 - val_loss: 0.6695 - val_accuracy: 0.6500\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 96.2596 - accuracy: 0.6778 - val_loss: 0.6690 - val_accuracy: 0.6500\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 206.0599 - accuracy: 0.6820 - val_loss: 0.6685 - val_accuracy: 0.6500\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 189.2525 - accuracy: 0.6527 - val_loss: 0.6680 - val_accuracy: 0.6500\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 65.3850 - accuracy: 0.6778 - val_loss: 0.6675 - val_accuracy: 0.6500\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 112.3682 - accuracy: 0.6527 - val_loss: 0.6670 - val_accuracy: 0.6500\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 132.4122 - accuracy: 0.6736 - val_loss: 0.6664 - val_accuracy: 0.6500\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 9ms/step - loss: 215.4829 - accuracy: 0.6611 - val_loss: 0.6659 - val_accuracy: 0.6500\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 34.8932 - accuracy: 0.6904 - val_loss: 0.6655 - val_accuracy: 0.6500\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 40.1145 - accuracy: 0.6611 - val_loss: 0.6650 - val_accuracy: 0.6500\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 57.9194 - accuracy: 0.6778 - val_loss: 0.6645 - val_accuracy: 0.6500\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 130.7071 - accuracy: 0.6695 - val_loss: 0.6640 - val_accuracy: 0.6500\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 82.9864 - accuracy: 0.6695 - val_loss: 0.6635 - val_accuracy: 0.6500\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 199.1843 - accuracy: 0.6611 - val_loss: 0.6630 - val_accuracy: 0.6500\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 124.9787 - accuracy: 0.6527 - val_loss: 0.6625 - val_accuracy: 0.6500\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 28.5361 - accuracy: 0.6904 - val_loss: 0.6621 - val_accuracy: 0.6500\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 49.4050 - accuracy: 0.6778 - val_loss: 0.6617 - val_accuracy: 0.6500\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 72.7824 - accuracy: 0.6946 - val_loss: 0.6614 - val_accuracy: 0.6500\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 113.8993 - accuracy: 0.6946 - val_loss: 0.6609 - val_accuracy: 0.6500\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 43.4422 - accuracy: 0.6820 - val_loss: 0.6605 - val_accuracy: 0.6500\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 43.0585 - accuracy: 0.6820 - val_loss: 0.6601 - val_accuracy: 0.6500\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 12.4297 - accuracy: 0.6820 - val_loss: 0.6597 - val_accuracy: 0.6500\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 59.1920 - accuracy: 0.6527 - val_loss: 0.6593 - val_accuracy: 0.6500\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 41.1239 - accuracy: 0.6778 - val_loss: 0.6589 - val_accuracy: 0.6500\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 21.8793 - accuracy: 0.6778 - val_loss: 0.6586 - val_accuracy: 0.6500\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 53.8132 - accuracy: 0.6653 - val_loss: 0.6583 - val_accuracy: 0.6500\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 28.0273 - accuracy: 0.6820 - val_loss: 0.6580 - val_accuracy: 0.6500\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 28.6432 - accuracy: 0.6736 - val_loss: 0.6577 - val_accuracy: 0.6500\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 30.9406 - accuracy: 0.6820 - val_loss: 0.6573 - val_accuracy: 0.6500\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 90.8422 - accuracy: 0.6695 - val_loss: 0.6570 - val_accuracy: 0.6500\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 131.7910 - accuracy: 0.6653 - val_loss: 0.6567 - val_accuracy: 0.6500\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 47.7301 - accuracy: 0.6820 - val_loss: 0.6564 - val_accuracy: 0.6500\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 103.6058 - accuracy: 0.6778 - val_loss: 0.6561 - val_accuracy: 0.6500\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 147.1279 - accuracy: 0.6527 - val_loss: 0.6558 - val_accuracy: 0.6500\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 21.0814 - accuracy: 0.6485 - val_loss: 0.6554 - val_accuracy: 0.6500\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 20.2239 - accuracy: 0.6862 - val_loss: 0.6551 - val_accuracy: 0.6500\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 29.4848 - accuracy: 0.6820 - val_loss: 0.6548 - val_accuracy: 0.6500\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 59.0009 - accuracy: 0.6862 - val_loss: 0.6545 - val_accuracy: 0.6500\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 6.3337 - accuracy: 0.6946 - val_loss: 0.6543 - val_accuracy: 0.6500\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 20.6376 - accuracy: 0.6778 - val_loss: 0.6541 - val_accuracy: 0.6500\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.7007 - accuracy: 0.6820 - val_loss: 0.6538 - val_accuracy: 0.6500\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 14.8406 - accuracy: 0.6904 - val_loss: 0.6536 - val_accuracy: 0.6500\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 137.2092 - accuracy: 0.6820 - val_loss: 0.6534 - val_accuracy: 0.6500\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6410 - accuracy: 0.6820 - val_loss: 0.6531 - val_accuracy: 0.6500\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 44.3532 - accuracy: 0.6611 - val_loss: 0.6529 - val_accuracy: 0.6500\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 175.6091 - accuracy: 0.6778 - val_loss: 0.6527 - val_accuracy: 0.6500\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 13.1528 - accuracy: 0.6862 - val_loss: 0.6525 - val_accuracy: 0.6500\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12.3112 - accuracy: 0.6820 - val_loss: 0.6523 - val_accuracy: 0.6500\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 63.2164 - accuracy: 0.6736 - val_loss: 0.6522 - val_accuracy: 0.6500\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 59.6612 - accuracy: 0.6695 - val_loss: 0.6520 - val_accuracy: 0.6500\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history_dropout = model_with_dropout.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75d45163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6367 - accuracy: 0.6862\n",
      "Training Accuracy (with dropout): 0.69\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6520 - accuracy: 0.6500\n",
      "Testing Accuracy (with dropout): 0.65\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "train_loss_dropout, train_accuracy_dropout = model_with_dropout.evaluate(x_train, y_train)\n",
    "print(f'Training Accuracy (with dropout): {train_accuracy_dropout:.2f}')\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "test_loss_dropout, test_accuracy_dropout = model_with_dropout.evaluate(x_test, y_test)\n",
    "print(f'Testing Accuracy (with dropout): {test_accuracy_dropout:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f891a01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 28.6687 - accuracy: 0.6820 - val_loss: 0.6518 - val_accuracy: 0.6500\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 15.9075 - accuracy: 0.6820 - val_loss: 0.6516 - val_accuracy: 0.6500\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26.3040 - accuracy: 0.6820 - val_loss: 0.6514 - val_accuracy: 0.6500\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 87.0774 - accuracy: 0.6736 - val_loss: 0.6513 - val_accuracy: 0.6500\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 34.3355 - accuracy: 0.6862 - val_loss: 0.6511 - val_accuracy: 0.6500\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 22.6067 - accuracy: 0.6778 - val_loss: 0.6509 - val_accuracy: 0.6500\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.6426 - accuracy: 0.6862 - val_loss: 0.6508 - val_accuracy: 0.6500\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.1018 - accuracy: 0.6904 - val_loss: 0.6506 - val_accuracy: 0.6500\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 29.6962 - accuracy: 0.6778 - val_loss: 0.6505 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8.6144 - accuracy: 0.6820 - val_loss: 0.6504 - val_accuracy: 0.6500\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.2092 - accuracy: 0.6904 - val_loss: 0.6502 - val_accuracy: 0.6500\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 31.9475 - accuracy: 0.6653 - val_loss: 0.6500 - val_accuracy: 0.6500\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.5663 - accuracy: 0.6862 - val_loss: 0.6499 - val_accuracy: 0.6500\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 32.1841 - accuracy: 0.6736 - val_loss: 0.6498 - val_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 18.5209 - accuracy: 0.6862 - val_loss: 0.6496 - val_accuracy: 0.6500\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3.2746 - accuracy: 0.6820 - val_loss: 0.6495 - val_accuracy: 0.6500\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 64.1938 - accuracy: 0.6653 - val_loss: 0.6494 - val_accuracy: 0.6500\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 126.1464 - accuracy: 0.6695 - val_loss: 0.6493 - val_accuracy: 0.6500\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.5527 - accuracy: 0.6862 - val_loss: 0.6492 - val_accuracy: 0.6500\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 46.0783 - accuracy: 0.6820 - val_loss: 0.6491 - val_accuracy: 0.6500\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 54.6446 - accuracy: 0.6653 - val_loss: 0.6490 - val_accuracy: 0.6500\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 61.1231 - accuracy: 0.6736 - val_loss: 0.6489 - val_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 19.8766 - accuracy: 0.6820 - val_loss: 0.6488 - val_accuracy: 0.6500\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12.1164 - accuracy: 0.6904 - val_loss: 0.6487 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6869 - accuracy: 0.6820 - val_loss: 0.6486 - val_accuracy: 0.6500\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26.4723 - accuracy: 0.6820 - val_loss: 0.6485 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6439 - accuracy: 0.6904 - val_loss: 0.6485 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 27.6531 - accuracy: 0.6778 - val_loss: 0.6484 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8.4022 - accuracy: 0.6904 - val_loss: 0.6483 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7888 - accuracy: 0.6862 - val_loss: 0.6483 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 31.3404 - accuracy: 0.6820 - val_loss: 0.6482 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 28.3785 - accuracy: 0.6820 - val_loss: 0.6482 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 65.3373 - accuracy: 0.6611 - val_loss: 0.6481 - val_accuracy: 0.6500\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 15.5096 - accuracy: 0.6862 - val_loss: 0.6481 - val_accuracy: 0.6500\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 17.5194 - accuracy: 0.6862 - val_loss: 0.6480 - val_accuracy: 0.6500\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.9104 - accuracy: 0.6862 - val_loss: 0.6479 - val_accuracy: 0.6500\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 20.8826 - accuracy: 0.6820 - val_loss: 0.6479 - val_accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 10.3243 - accuracy: 0.6862 - val_loss: 0.6479 - val_accuracy: 0.6500\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 37.8648 - accuracy: 0.6904 - val_loss: 0.6478 - val_accuracy: 0.6500\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 31.8180 - accuracy: 0.6820 - val_loss: 0.6478 - val_accuracy: 0.6500\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 30.2959 - accuracy: 0.6862 - val_loss: 0.6478 - val_accuracy: 0.6500\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 8.7007 - accuracy: 0.6862 - val_loss: 0.6477 - val_accuracy: 0.6500\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8.3349 - accuracy: 0.6904 - val_loss: 0.6477 - val_accuracy: 0.6500\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.7371 - accuracy: 0.6820 - val_loss: 0.6476 - val_accuracy: 0.6500\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 64.8665 - accuracy: 0.6653 - val_loss: 0.6476 - val_accuracy: 0.6500\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5.0944 - accuracy: 0.6862 - val_loss: 0.6476 - val_accuracy: 0.6500\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 76.6344 - accuracy: 0.6820 - val_loss: 0.6476 - val_accuracy: 0.6500\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.3136 - accuracy: 0.6778 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 10.1567 - accuracy: 0.6862 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.7421 - accuracy: 0.6862 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6222 - accuracy: 0.6862 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3.9630 - accuracy: 0.6946 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 73.8511 - accuracy: 0.6862 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 15.4837 - accuracy: 0.6778 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 11.5450 - accuracy: 0.6820 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 16.5776 - accuracy: 0.6820 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 54.4233 - accuracy: 0.6820 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 11.7531 - accuracy: 0.6820 - val_loss: 0.6474 - val_accuracy: 0.6500\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 93.8500 - accuracy: 0.6820 - val_loss: 0.6474 - val_accuracy: 0.6500\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 34.9664 - accuracy: 0.6778 - val_loss: 0.6474 - val_accuracy: 0.6500\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.8501 - accuracy: 0.6904 - val_loss: 0.6474 - val_accuracy: 0.6500\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.5301 - accuracy: 0.6778 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.0478 - accuracy: 0.6862 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0413 - accuracy: 0.6862 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 39.0634 - accuracy: 0.6862 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.7906 - accuracy: 0.6820 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8.8436 - accuracy: 0.6820 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 37.5320 - accuracy: 0.6820 - val_loss: 0.6475 - val_accuracy: 0.6500\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.6783 - accuracy: 0.6820 - val_loss: 0.6475 - val_accuracy: 0.6500\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define Early Stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history_early_stopping = model_with_dropout.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef4cdd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6251 - accuracy: 0.6862\n",
      "Training Accuracy (with early stopping): 0.69\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.6474 - accuracy: 0.6500\n",
      "Testing Accuracy (with early stopping): 0.65\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data with early stopping\n",
    "train_loss_early_stopping, train_accuracy_early_stopping = model_with_dropout.evaluate(x_train, y_train)\n",
    "print(f'Training Accuracy (with early stopping): {train_accuracy_early_stopping:.2f}')\n",
    "\n",
    "# Evaluate the model on testing data with early stopping\n",
    "test_loss_early_stopping, test_accuracy_early_stopping = model_with_dropout.evaluate(x_test, y_test)\n",
    "print(f'Testing Accuracy (with early stopping): {test_accuracy_early_stopping:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d00db",
   "metadata": {},
   "source": [
    "The training and testing accuracies with early stopping are similar to the accuracies without early stopping. This suggests that early stopping might not have a significant impact on the model's performance for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "781bc924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 64)                832       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2945 (11.50 KB)\n",
      "Trainable params: 2945 (11.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model with increased number of neurons\n",
    "model_with_increased_neurons = keras.Sequential([\n",
    "    keras.layers.Dense(units=64, activation='relu', input_dim=12),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(units=32, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_with_increased_neurons.compile(optimizer='adam',\n",
    "                                     loss='binary_crossentropy',\n",
    "                                     metrics=['accuracy'])\n",
    "\n",
    "# Print the summary of the model\n",
    "print(model_with_increased_neurons.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36e438d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: 8160.9556 - accuracy: 0.4812 - val_loss: 2797.1890 - val_accuracy: 0.6500\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 5990.1885 - accuracy: 0.6067 - val_loss: 3754.5110 - val_accuracy: 0.6500\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4785.5674 - accuracy: 0.5565 - val_loss: 1630.4297 - val_accuracy: 0.6500\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 4138.1489 - accuracy: 0.5816 - val_loss: 846.9542 - val_accuracy: 0.6500\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5494.1787 - accuracy: 0.4854 - val_loss: 858.4142 - val_accuracy: 0.6500\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 4447.7534 - accuracy: 0.5146 - val_loss: 1171.3201 - val_accuracy: 0.6500\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 4536.7266 - accuracy: 0.5063 - val_loss: 1320.8575 - val_accuracy: 0.6500\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 3524.5310 - accuracy: 0.5941 - val_loss: 784.0955 - val_accuracy: 0.6500\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3585.8955 - accuracy: 0.5188 - val_loss: 759.0715 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2471.8572 - accuracy: 0.5983 - val_loss: 1184.9470 - val_accuracy: 0.6500\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2504.1624 - accuracy: 0.5900 - val_loss: 1031.2335 - val_accuracy: 0.6500\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 2299.3049 - accuracy: 0.5816 - val_loss: 724.3239 - val_accuracy: 0.6500\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1994.1537 - accuracy: 0.5397 - val_loss: 384.8839 - val_accuracy: 0.6500\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1710.5900 - accuracy: 0.5314 - val_loss: 726.8109 - val_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1383.5848 - accuracy: 0.5774 - val_loss: 594.8621 - val_accuracy: 0.6500\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1341.8859 - accuracy: 0.5816 - val_loss: 399.8559 - val_accuracy: 0.6500\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1533.7076 - accuracy: 0.5565 - val_loss: 117.9969 - val_accuracy: 0.6500\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1319.4497 - accuracy: 0.5690 - val_loss: 98.0579 - val_accuracy: 0.6500\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1294.9316 - accuracy: 0.5732 - val_loss: 84.6847 - val_accuracy: 0.6500\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1053.1289 - accuracy: 0.6067 - val_loss: 97.7661 - val_accuracy: 0.6500\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 962.7921 - accuracy: 0.5732 - val_loss: 130.8973 - val_accuracy: 0.6500\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1042.6724 - accuracy: 0.5272 - val_loss: 138.1758 - val_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 743.5315 - accuracy: 0.5439 - val_loss: 208.6484 - val_accuracy: 0.6500\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 643.7549 - accuracy: 0.6234 - val_loss: 236.7739 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 764.2072 - accuracy: 0.6234 - val_loss: 181.8655 - val_accuracy: 0.6500\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 649.1705 - accuracy: 0.5690 - val_loss: 119.6355 - val_accuracy: 0.6500\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 637.2772 - accuracy: 0.5732 - val_loss: 92.8552 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 626.6552 - accuracy: 0.5272 - val_loss: 67.3035 - val_accuracy: 0.6500\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 437.8416 - accuracy: 0.5690 - val_loss: 28.4267 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 526.7023 - accuracy: 0.5356 - val_loss: 8.0796 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 399.2114 - accuracy: 0.5356 - val_loss: 7.2620 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 430.5458 - accuracy: 0.5356 - val_loss: 6.1668 - val_accuracy: 0.3500\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 298.4703 - accuracy: 0.5356 - val_loss: 15.9916 - val_accuracy: 0.3500\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 255.9754 - accuracy: 0.5941 - val_loss: 3.2106 - val_accuracy: 0.6500\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 219.7369 - accuracy: 0.6151 - val_loss: 2.8969 - val_accuracy: 0.6500\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 279.6496 - accuracy: 0.6109 - val_loss: 0.7943 - val_accuracy: 0.6500\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 206.0252 - accuracy: 0.6025 - val_loss: 0.6911 - val_accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 281.6379 - accuracy: 0.5105 - val_loss: 0.8333 - val_accuracy: 0.6500\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 216.0258 - accuracy: 0.6318 - val_loss: 1.9568 - val_accuracy: 0.3500\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 159.9869 - accuracy: 0.5146 - val_loss: 1.9767 - val_accuracy: 0.6500\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 79.1695 - accuracy: 0.5732 - val_loss: 7.9540 - val_accuracy: 0.3500\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 113.9106 - accuracy: 0.5188 - val_loss: 1.4553 - val_accuracy: 0.6500\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 106.0153 - accuracy: 0.6192 - val_loss: 2.4649 - val_accuracy: 0.3500\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 113.8194 - accuracy: 0.5732 - val_loss: 1.0660 - val_accuracy: 0.3500\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 111.0756 - accuracy: 0.6444 - val_loss: 0.8266 - val_accuracy: 0.3500\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 85.9250 - accuracy: 0.6109 - val_loss: 0.9284 - val_accuracy: 0.6500\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 78.9275 - accuracy: 0.6234 - val_loss: 0.6602 - val_accuracy: 0.6500\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 88.7678 - accuracy: 0.6402 - val_loss: 1.0746 - val_accuracy: 0.6500\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 61.7430 - accuracy: 0.6109 - val_loss: 0.8492 - val_accuracy: 0.6500\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 47.7880 - accuracy: 0.6192 - val_loss: 1.2515 - val_accuracy: 0.3500\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 85.6786 - accuracy: 0.5983 - val_loss: 0.6716 - val_accuracy: 0.6500\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 69.8251 - accuracy: 0.5649 - val_loss: 0.6478 - val_accuracy: 0.6500\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 47.0568 - accuracy: 0.6025 - val_loss: 1.1882 - val_accuracy: 0.6500\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 56.3078 - accuracy: 0.5146 - val_loss: 0.6557 - val_accuracy: 0.6500\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 49.3784 - accuracy: 0.6192 - val_loss: 0.6808 - val_accuracy: 0.6500\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 54.0532 - accuracy: 0.5858 - val_loss: 1.0093 - val_accuracy: 0.3500\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 86.0318 - accuracy: 0.5523 - val_loss: 0.9412 - val_accuracy: 0.6500\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 7ms/step - loss: 88.2613 - accuracy: 0.4979 - val_loss: 0.6211 - val_accuracy: 0.6500\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 35.0273 - accuracy: 0.6736 - val_loss: 0.7529 - val_accuracy: 0.3500\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 44.5373 - accuracy: 0.6109 - val_loss: 0.6749 - val_accuracy: 0.6500\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 80.8333 - accuracy: 0.5272 - val_loss: 0.6323 - val_accuracy: 0.6500\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 45.9787 - accuracy: 0.6611 - val_loss: 0.8381 - val_accuracy: 0.3500\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 50.4035 - accuracy: 0.5523 - val_loss: 0.6181 - val_accuracy: 0.6500\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 22.4052 - accuracy: 0.6067 - val_loss: 0.7469 - val_accuracy: 0.6500\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 32.7432 - accuracy: 0.6025 - val_loss: 0.7691 - val_accuracy: 0.3500\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 42.7047 - accuracy: 0.6360 - val_loss: 0.7903 - val_accuracy: 0.3500\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 37.2632 - accuracy: 0.6444 - val_loss: 0.7071 - val_accuracy: 0.2833\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 29.3082 - accuracy: 0.5607 - val_loss: 0.6304 - val_accuracy: 0.6500\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 19.9008 - accuracy: 0.5941 - val_loss: 0.6737 - val_accuracy: 0.6500\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 25.1723 - accuracy: 0.6653 - val_loss: 0.7472 - val_accuracy: 0.3333\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 34.5953 - accuracy: 0.6025 - val_loss: 0.6374 - val_accuracy: 0.6500\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 41.6869 - accuracy: 0.6234 - val_loss: 0.7265 - val_accuracy: 0.3333\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12.7746 - accuracy: 0.5858 - val_loss: 0.6246 - val_accuracy: 0.6500\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6339 - accuracy: 0.6862\n",
      "Training Accuracy (with increased neurons): 0.69\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.6181 - accuracy: 0.6500\n",
      "Testing Accuracy (with increased neurons): 0.65\n"
     ]
    }
   ],
   "source": [
    "# Train the model with increased number of neurons\n",
    "history_increased_neurons = model_with_increased_neurons.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on training data with increased number of neurons\n",
    "train_loss_increased_neurons, train_accuracy_increased_neurons = model_with_increased_neurons.evaluate(x_train, y_train)\n",
    "print(f'Training Accuracy (with increased neurons): {train_accuracy_increased_neurons:.2f}')\n",
    "\n",
    "# Evaluate the model on testing data with increased number of neurons\n",
    "test_loss_increased_neurons, test_accuracy_increased_neurons = model_with_increased_neurons.evaluate(x_test, y_test)\n",
    "print(f'Testing Accuracy (with increased neurons): {test_accuracy_increased_neurons:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfefc903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 69.4396 - accuracy: 0.6527 - val_loss: 0.7889 - val_accuracy: 0.3500\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 109.1095 - accuracy: 0.6234 - val_loss: 1.2367 - val_accuracy: 0.3500\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 72.3144 - accuracy: 0.5397 - val_loss: 0.7574 - val_accuracy: 0.6500\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 28.8762 - accuracy: 0.5941 - val_loss: 0.7155 - val_accuracy: 0.3333\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7.9533 - accuracy: 0.6151 - val_loss: 0.6412 - val_accuracy: 0.6500\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 43.2418 - accuracy: 0.6234 - val_loss: 1.1018 - val_accuracy: 0.3500\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 31.0513 - accuracy: 0.6360 - val_loss: 0.6370 - val_accuracy: 0.6500\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 16.9994 - accuracy: 0.5900 - val_loss: 0.7244 - val_accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 35.7062 - accuracy: 0.5690 - val_loss: 0.6197 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 36.6638 - accuracy: 0.6025 - val_loss: 0.6634 - val_accuracy: 0.6500\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.9437 - accuracy: 0.6360 - val_loss: 0.6690 - val_accuracy: 0.6500\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.6607 - accuracy: 0.6360 - val_loss: 0.6444 - val_accuracy: 0.6500\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26.6806 - accuracy: 0.6527 - val_loss: 0.6980 - val_accuracy: 0.4667\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 28.5628 - accuracy: 0.6192 - val_loss: 0.6590 - val_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 20.9004 - accuracy: 0.5523 - val_loss: 0.6246 - val_accuracy: 0.6500\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 34.4169 - accuracy: 0.6402 - val_loss: 0.8093 - val_accuracy: 0.3500\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 19.5939 - accuracy: 0.6151 - val_loss: 0.6213 - val_accuracy: 0.6500\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 17.7745 - accuracy: 0.6611 - val_loss: 0.6733 - val_accuracy: 0.6500\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13.1713 - accuracy: 0.5941 - val_loss: 0.6467 - val_accuracy: 0.6500\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6309 - accuracy: 0.6862\n",
      "Training Accuracy (with increased neurons): 0.69\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.6197 - accuracy: 0.6500\n",
      "Testing Accuracy (with increased neurons): 0.65\n"
     ]
    }
   ],
   "source": [
    "# Train the model with increased number of neurons\n",
    "history_increased_neurons = model_with_increased_neurons.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on training data with increased number of neurons\n",
    "train_loss_increased_neurons, train_accuracy_increased_neurons = model_with_increased_neurons.evaluate(x_train, y_train)\n",
    "print(f'Training Accuracy (with increased neurons): {train_accuracy_increased_neurons:.2f}')\n",
    "\n",
    "# Evaluate the model on testing data with increased number of neurons\n",
    "test_loss_increased_neurons, test_accuracy_increased_neurons = model_with_increased_neurons.evaluate(x_test, y_test)\n",
    "print(f'Testing Accuracy (with increased neurons): {test_accuracy_increased_neurons:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ad81c",
   "metadata": {},
   "source": [
    "The training and testing accuracies are similar to the previous model's performance, indicating that increasing the number of neurons did not lead to significant improvement in generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468cf481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ae94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc527616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4afbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
